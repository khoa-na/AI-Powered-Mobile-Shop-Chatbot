# -*- coding: utf-8 -*-
"""load_model_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1s0D9_mHjZ2v7fOBUFuDPlF8MM5jt2B0x
"""

import os
import pandas as pd
import json
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from keras.models import Model
from keras.layers import LSTM, Activation, Dense, Dropout, Input, Embedding
from keras.preprocessing.sequence import pad_sequences
from keras import layers, activations, models, preprocessing, utils
from keras.optimizers import RMSprop
from keras.preprocessing.text import Tokenizer
from keras.preprocessing import sequence
from nltk.tokenize import word_tokenize, sent_tokenize
from keras.optimizers import Adam
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix
import nltk
import sqlite3
from nltk import pos_tag
# nltk.download('punkt')
# nltk.download('averaged_perceptron_tagger')
# nltk.download('punkt')

model_1= tf.keras.models.load_model('models/classification.h5',compile=False)
model_2 = tf.keras.models.load_model('models/chatbot.h5',compile=False)

# with open('/content/classification_v3.json', 'r') as j:
#      contents = json.loads(j.read())

def preprocess_and_tokenize_sentences(text):
    text = text.lower()
    # text = re.sub(r'[^a-zA-Z\s?.]', '', text)
    text = text.replace('?','')
    sentences = sent_tokenize(text)
    return sentences

# tags_list=[]
# sentences_list= []
# for intents in contents['Intenst']:
#     tags= intents['Tags']
#     questions = intents['Questions']
#     for question in questions:
#         tags_list.append(tags)
#         sentences = preprocess_and_tokenize_sentences(question)
#         # print(sentences)
#         sentences_list.extend(sentences)

chatbot_data= pd.read_csv('data/chatbot_dataset_5.csv')
chatbot_data['cauhoi'] = chatbot_data['cauhoi'].astype(str)
chatbot_data['traloi'] = chatbot_data['traloi'].astype(str)
questions= chatbot_data['cauhoi'].tolist()

# classification_data = pd.DataFrame({'Sentence': sentences_list,'Tag': tags_list})
# classification_data = classification_data.sample(frac=1, random_state=42).reset_index(drop=True)
# classification_data.to_csv('classification.csv',index=False)

classification_data = pd.read_csv('data/classification_v3.csv')
classification_data.head()

labels_mapping = {
    'Informations tags': 1,
    'Non-Informations tags': 0
}
classification_data['Tag'] = classification_data['Tag'].map(labels_mapping)

X = classification_data.Sentence
Y = classification_data.Tag
X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.1, random_state=42)
classification_data.sample(10)

all_texts = classification_data['Sentence'].tolist() + chatbot_data['cauhoi'].tolist() + chatbot_data['traloi'].tolist()

max_len = 18
tokenizer = Tokenizer(oov_token='OOV')
tokenizer.fit_on_texts(all_texts)
X_train_sequences = tokenizer.texts_to_sequences(X_train)
# print(X_train_sequences)
X_test_sequences = tokenizer.texts_to_sequences(X_test)
vocab_size = len(tokenizer.word_index) + 1
X_train_padded = pad_sequences(X_train_sequences, maxlen=max_len, padding='post')
X_test_padded = pad_sequences(X_test_sequences, maxlen=max_len, padding='post')

questions = chatbot_data['cauhoi'].tolist()
answers = chatbot_data["traloi"].to_list()
answers_with_tags = list()
#lấy ra từ điển
for i in range( len( answers ) ):
    if type( answers[i] ) == str:
        answers_with_tags.append( answers[i] )
    else:
        questions.pop( i )
answers = list()

#gan nhan
for i in range( len( answers_with_tags ) ) :
    answers.append( '<START> ' + answers_with_tags[i] + ' <END>' )

# tokenizer = preprocessing.text.Tokenizer()
# tokenizer.fit_on_texts( questions + answers )
VOCAB_SIZE = len( tokenizer.word_index ) + 1
# print( 'Số lượng từ trong từ điển: {}'.format( VOCAB_SIZE ))

#trích thành các từ xl các kí tự đặc biệt
vocab = []
for word in tokenizer.word_index:
  vocab.append(word)

tokenized_questions = tokenizer.texts_to_sequences( questions )
maxlen_questions = max( [len(x) for x in tokenized_questions ] )
padded_questions = preprocessing.sequence.pad_sequences( tokenized_questions, maxlen = maxlen_questions, padding = 'post')
encoder_input_data = np.array(padded_questions)

tokenized_answers = tokenizer.texts_to_sequences( answers )
maxlen_answers = max( [ len(x) for x in tokenized_answers ] )
padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )
decoder_input_data = np.array( padded_answers )
# print(  )
# print("Decoder_input_data",decoder_input_data.shape , maxlen_answers)
# print(answers[0])
# print(decoder_input_data[0])

# decoder_output_data giải mã từ vector số sang string
tokenized_answers = tokenizer.texts_to_sequences( answers )
for i in range(len(tokenized_answers)) :
    tokenized_answers[i] = tokenized_answers[i][1:]
padded_answers = preprocessing.sequence.pad_sequences( tokenized_answers , maxlen=maxlen_answers , padding='post' )
onehot_answers = utils.to_categorical( padded_answers , VOCAB_SIZE )
decoder_output_data = np.array( onehot_answers )

#model_2.summary()

def make_inference_models():
  encoder_inputs=model_2.input[0] #input_1
  encoder_output, state_h_enc, state_c_enc = model_2.layers[4].output
  encoder_states = [state_h_enc,state_c_enc]
  encoder_model= Model(encoder_inputs,encoder_states)

  decoder_inputs = model_2.input[1]
  decoder_state_input_h = tf.keras.layers.Input(shape=(200,))
  decoder_state_input_c = tf.keras.layers.Input(shape=(200,))
  decoder_state_inputs= [decoder_state_input_h,decoder_state_input_c]
  decoder_embedding = model_2.layers[3](decoder_inputs)

  decoder_lstm = model_2.layers[5]
  decoder_outputs, state_h_dec, state_c_dec= decoder_lstm(decoder_embedding,initial_state=decoder_state_inputs)
  decoder_states= [state_h_dec,state_c_dec]

  decoder_dense = model_2.layers[6]
  decoder_outputs = decoder_dense(decoder_outputs)
  decoder_model = Model([decoder_inputs]+decoder_state_inputs,
                        [decoder_outputs]+ decoder_states)
  return encoder_model, decoder_model

def str_to_tokens( sentence : str ):

    words = sentence.lower().replace("?","").split()
    tokens_list = list()

    for word in words:
        try:
            tokens_list.append( tokenizer.word_index[ word ] )
        except:
            tokens_list.append(tokenizer.word_index['i dont know'])

    return preprocessing.sequence.pad_sequences( [tokens_list] , maxlen=maxlen_questions , padding='post')

enc_model , dec_model = make_inference_models()

def chat_bot_lstm(cau_hoi):
    states_values = enc_model.predict( str_to_tokens( cau_hoi ) ,verbose=0)
#     print(states_values)
    empty_target_seq = np.zeros( ( 1 , 1 ) )
    empty_target_seq[0, 0] = tokenizer.word_index['start']
    stop_condition = False
    decoded_translation = ''
    while not stop_condition :
        dec_outputs , h , c = dec_model.predict([ empty_target_seq ] + states_values ,verbose=0)
        sampled_word_index = np.argmax( dec_outputs[0, -1, :] )
        sampled_word = None
        for word , index in tokenizer.word_index.items() :
            if sampled_word_index == index :
                decoded_translation += ' {}'.format( word )
                sampled_word = word
        if sampled_word == 'end' or len(decoded_translation.split()) > maxlen_answers:
            stop_condition = True
        empty_target_seq = np.zeros( ( 1 , 1 ) )
        empty_target_seq[ 0 , 0 ] = sampled_word_index
        states_values = [ h , c ]
    return ( decoded_translation[1:-4])

def prediction(user_input_padded,thershold=0.002):
    predict = model_1.predict(user_input_padded)
    #print(user_input_padded)
    #print(predict)
    if predict[0] >= thershold:
        return 'Information Tag'
    else:
        return 'Non-Information Tag'

def remove_padding(input_padded):
    unpadded_input = []
    for sequence in input_padded:
        sentence = [word for word in sequence if word != 0]  # Assuming 0 is the padding token
        # print(sentence)
        unpadded_input.append(sentence)
    return unpadded_input

# Reverse the tokenization
def reverse_tokenization(sequences, tokenizer):
    original_sentences = []
    for sequence in sequences:
        # print(sequence)
        original_sentence = tokenizer.sequences_to_texts([sequence])[0]
        # print(original_sentence)
        original_sentences.append(original_sentence)
    return original_sentences

def pos_tokenize(text):
  words = word_tokenize(text)
  tags = pos_tag(words)
  selected_words = [word for word, tag in tags if tag in ['NN', 'NNP','CD','JJ']]
  return selected_words

def classification_tags(predict, temp):
  if predict == 'Information Tag':
      original_user_input = temp
      price_tags = [
    'cost', 'price', 'how much', 'sell', 'budget', 'quotation', 'amount',"lowest price","total"
    'market value', 'pricing information', 'price list', 'price range',"final cost","sale price"
    'purchase cost', 'price point', 'expense', 'valuation',"rate", "cost range","price estimation"
    'invoice','list price', 'top price', 'bottom price',"retail price","expenditure"
    'final price', 'bill',"sold","purchase","spend","pay","price tag","current price",'much'
]
      screen_resolution_tags = [
    "display size",
    "screen dimensions",
    "diagonal length",
    "inch measurement",
    "display diagonal",
    "screen diagonal",
    "physical size",
    "display dimensions",
    "visual display size",
    "usable screen space",
    "full screen size",
    "touchscreen size",
    "screen size",
    "display area",
    'phone size',
    "phone display",
    "physical screen size",
    "touchscreen size",
    "screen",
    "size"

]
      phone_memory_keywords = [
    "internal storage",
    "built-in storage",
    "storage capacity",
    "memory space",
    "onboard storage",
    "ROM",
    "flash storage",
    "digital storage",
    "data storage capacity",
    "storage",
    "memory",
    "resolution",
    'size'
]

      # information_tags = ['information']
      tokenize = pos_tokenize(original_user_input)
      # print(tokenize)
      for keyword in tokenize:
        if keyword in price_tags:
          filtered_elements = [element for element in tokenize if element not in price_tags]
          concatenated_string = ' '.join(filtered_elements)
          return 'price_tag', concatenated_string
        elif keyword in screen_resolution_tags:
          filtered_elements = [element for element in tokenize if element not in screen_resolution_tags]
          concatenated_string = ' '.join(filtered_elements)
          return 'resolution_tag', concatenated_string
        elif keyword in phone_memory_keywords:
          filtered_elements = [element for element in tokenize if element not in phone_memory_keywords]
          concatenated_string = ' '.join(filtered_elements)
          return 'memory_tag', concatenated_string
  else:
    original_user_input = temp
    return "Non-Information Tag",chat_bot_lstm(original_user_input)

def last(user_type):
    user_input = user_type
    temp = user_type
    # Assume preprocess_and_tokenize_sentences, pad_sequences, and prediction functions are defined elsewhere
    user_input_processed = preprocess_and_tokenize_sentences(user_input)
    user_input_sequences = tokenizer.texts_to_sequences(user_input_processed)
    user_input_padded = pad_sequences(user_input_sequences, maxlen=max_len, padding='post')
    # Get the prediction
    predict = prediction(user_input_padded)
    # Call the classification_tags function with the prediction
    tag_result = classification_tags(predict, temp)
    return predict, tag_result
